# ğŸ” CodeQuery API

**AI-Powered Code Intelligence** â€“ Ingest GitHub repositories and ask natural-language questions about the code using Retrieval-Augmented Generation (RAG).

> COMP3011 â€“ Web Services and Web Data | Sachin Sindhe

---

## ğŸ—ï¸ Architecture

```
Client (Postman / Frontend)
    â”‚  HTTP/JSON
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  FastAPI (async)        â”‚ â† API Layer
â”‚  â”œâ”€â”€ /repositories      â”‚
â”‚  â”œâ”€â”€ /questions         â”‚
â”‚  â””â”€â”€ /analysis          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ PostgreSQL  â”‚    â”‚ Celery +     â”‚
  â”‚ + pgvector  â”‚    â”‚ Redis        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                     â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  Ollama LLM  â”‚
                     â”‚  (Mistral)   â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites
- Docker & Docker Compose

### 1. Clone & Configure

```bash
git clone <your-repo-url>
cd codequery-api
cp .env.example .env   # Edit if needed
```

### 2. Start Services (Lightweight Mode by Default)

CodeQuery is configured to run in **Lightweight Mode** by default (< 20GB storage footprint). This runs only the API and PostgreSQL Database. Ingestion is handled natively within the API.

```bash
# Starts API + DB only
docker compose up --build -d
```

**âš ï¸ Warning on Storage Limits:** In `Lightweight Mode`, please start by testing ingestion on tiny repositories (e.g., https://github.com/octocat/Hello-World) to prevent memory blocks.

### 3. Optional Advanced Profiles
If you have sufficient RAM and storage, you can enable advanced services using Docker Compose profiles.

* **Async Mode (Celery + Redis):**
  Offloads ingestion to a background worker queue.
  ```bash
  docker compose --profile async up --build -d
  ```

* **LLM Mode (Local Ollama LLM):**
  Allows Q&A inference locally.
  ```bash
  docker compose --profile llm up --build -d
  
  # Pull the model once after it's started:
  docker exec -it codequery-ollama ollama pull mistral
  ```

* **Full Mode (All Services):**
  ```bash
  docker compose --profile async --profile llm up --build -d
  ```

### 4. Explore the API

- **Swagger UI:** [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc:** [http://localhost:8000/redoc](http://localhost:8000/redoc)
- **Health Check:** [http://localhost:8000/api/v1/health](http://localhost:8000/api/v1/health)

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/v1/repositories` | Add a repository for analysis |
| `GET` | `/api/v1/repositories` | List all repositories |
| `GET` | `/api/v1/repositories/{id}` | Get repository details |
| `PUT` | `/api/v1/repositories/{id}` | Update / re-analyse |
| `DELETE` | `/api/v1/repositories/{id}` | Delete repository |
| `POST` | `/api/v1/repositories/{id}/questions` | Ask a question |
| `GET` | `/api/v1/repositories/{id}/questions` | Question history |
| `GET` | `/api/v1/questions/{id}` | Get single question |
| `DELETE` | `/api/v1/questions/{id}` | Delete question |
| `GET` | `/api/v1/repositories/{id}/analysis` | Analysis status |
| `GET` | `/api/v1/repositories/{id}/statistics` | Code statistics |
| `GET` | `/api/v1/health` | Health check |

### Example: Add a Repository

```bash
curl -X POST http://localhost:8000/api/v1/repositories \
  -H "Content-Type: application/json" \
  -d '{
    "repo_url": "https://github.com/pallets/flask",
    "branch": "main",
    "name": "Flask"
  }'
```

### Example: Ask a Question

```bash
curl -X POST http://localhost:8000/api/v1/repositories/1/questions \
  -H "Content-Type: application/json" \
  -d '{
    "question": "How is routing implemented?"
  }'
```

---

## ğŸ—‚ï¸ Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”œâ”€â”€ config.py            # Pydantic Settings
â”‚   â”œâ”€â”€ database.py          # Async SQLAlchemy engine
â”‚   â”œâ”€â”€ api/                 # REST endpoints
â”‚   â”‚   â”œâ”€â”€ repositories.py  # CRUD for repos
â”‚   â”‚   â”œâ”€â”€ questions.py     # Q&A endpoints
â”‚   â”‚   â””â”€â”€ analysis.py      # Analysis status & stats
â”‚   â”œâ”€â”€ models/              # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ repository.py
â”‚   â”‚   â”œâ”€â”€ code_chunk.py
â”‚   â”‚   â”œâ”€â”€ question.py
â”‚   â”‚   â””â”€â”€ analysis_job.py
â”‚   â”œâ”€â”€ schemas/             # Pydantic request/response
â”‚   â”‚   â”œâ”€â”€ repository.py
â”‚   â”‚   â”œâ”€â”€ question.py
â”‚   â”‚   â””â”€â”€ analysis.py
â”‚   â”œâ”€â”€ services/            # Business logic
â”‚   â”‚   â”œâ”€â”€ ingestion.py     # Git clone + file discovery
â”‚   â”‚   â”œâ”€â”€ chunking.py      # Code parsing & chunking
â”‚   â”‚   â”œâ”€â”€ embedding.py     # Vector embeddings (free model)
â”‚   â”‚   â””â”€â”€ qa_engine.py     # RAG pipeline
â”‚   â””â”€â”€ tasks/               # Celery background tasks
â”‚       â””â”€â”€ analysis.py      # Full analysis pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_services.py
â”œâ”€â”€ alembic/                 # Database migrations
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env
```

---

## ğŸ§ª Running Tests

```bash
# Run pytest in the API container:
docker compose exec api pytest -q

# Or locally with a virtual environment
pytest tests/ -v --cov=app
```

---

## ğŸ”§ Database Migrations

```bash
# Generate a new migration
docker exec -it codequery-api alembic revision --autogenerate -m "description"

# Apply migrations
docker exec -it codequery-api alembic upgrade head
```

---

## ğŸ¤– AI Models Used

| Component | Model | Cost | Dimension |
|-----------|-------|------|-----------|
| **Embeddings** | `all-MiniLM-L6-v2` (sentence-transformers) | Free | 384 |
| **LLM** | Mistral via Ollama | Free (local) | â€“ |

> To switch to OpenAI, set `OPENAI_API_KEY` in `.env`.

---

## ğŸ“„ License

MIT License â€“ Built for COMP3011 coursework.
